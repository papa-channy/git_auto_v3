diff --git a/scripts/mm_gen.py b/scripts/mm_gen.py
index 25e58d2..e2c8db8 100644
--- a/scripts/mm_gen.py
+++ b/scripts/mm_gen.py
@@ -1,170 +1,164 @@
-from datetime import datetime
-import os
 import json
+import uuid
 import pandas as pd
-import tiktoken
-from scripts.dataframe import (
-    load_df, save_df,
-    REPO_PATH, INFO_PATH, STRATEGY_PATH, PROMPT_PATH,
-    init_prompt_df
-)
-from scripts.llm_router import call_llm
+from scripts.dataframe import load_df, save_df
+from utils.cfg import cfg
+from scripts.llm_mng import LLMManager
+from pathlib import Path
+
+CHUNK_THRESHOLDS = [(50, 3), (20, 2)]
 
 
 def split_chunks(lst, n):
     avg = len(lst) / float(n)
-    out = []
-    last = 0.0
+    out, last = [], 0.0
     while last < len(lst):
         out.append(lst[int(last):int(last + avg)])
         last += avg
     return out
 
 
-def log(message: str, log_file: str):
-    with open(log_file, "a", encoding="utf-8") as f:
-        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {message}\n")
-
-
-def build_strategy_pp(repo_df, info_df, strategy_df, file_chunk):
-    files_info = info_df[info_df["FILE"].isin(file_chunk)].to_dict(orient="records")
-    readme_map = {row["FILE"]: row["readme strategy"] for _, row in strategy_df.iterrows()}
-    readme_summary = {f["FILE"]: readme_map[f["FILE"]] for f in file_chunk}
-    recent_commits = info_df[info_df["FILE"].isin(file_chunk)]["5 LATEST COMMIT"].to_dict()
-
-    prompt = f"""
-Objective:
-For each modified file, predict the following information in JSON format:
-- Required Commit Detail (int, 1~5)
-- Component Type
-- Importance (int, 0~10)
-- Most Related Files (list[str], up to 3)
-
-JSON Example:
+def clean_llm_response(response: str) -> str:
+    lines = response.strip().splitlines()
+    lines = [line for line in lines if not line.strip().startswith(("```", "'''"))]
+    return "\n".join(lines).strip()
+
+
+def build_strategy_prompt(repo_df, info_df, strategy_df, file_chunk, id_map):
+    files_info = []
+    for file in file_chunk:
+        info_row = info_df[info_df["file"] == file].iloc[0]
+        strategy_row = strategy_df[strategy_df["File"] == file].iloc[0]
+        files_info.append({
+            "file": file,
+            "id": id_map[file],
+            "file token": info_row["file token"],
+            "diff token": info_row["diff token"],
+            "Readme strategy": strategy_row["Readme strategy"],
+            "5 latest commit": info_row["5 latest commit"]
+        })
+
+    prompt = f"""ğŸ“Œ Objective:
+For each modified file, predict the following information in **valid JSON array format**:
+
+Each result must correspond to a file from the list below:
+{file_chunk}
+
+Each item must include:
+- "id": what I give you
+- "File": exact filename from the list above
+- "Required Commit Detail" (int, 1~5)
+- "Component Type" (str)
+- "Importance" (int, 0~10)
+- "Most Related Files": up to 3 other files from the list (not including itself)
+
+âœ… Output Format (required):
+List[Dict], e.g.
 [
   {{
-    "file": "ext_info.py",
+    "id": "uuid-string",
+    "File": "ext_info.py",
     "Required Commit Detail": 4,
     "Component Type": "support",
     "Importance": 9,
     "Most Related Files": ["dataframe.py", "llm_router.py", "gen_msg.py"]
-  }},
-  ...
-]
-
-Output Format (follow strictly):
-[
-  {{
-    "file": str,
-    "Required Commit Detail": int,
-    "Component Type": str,
-    "Importance": int,
-    "Most Related Files": list[str]
-  }},
-  ...
+  }}
 ]
 
-Reference for Required Commit Detail:
-- Main branch: {repo_df["Main branch"].iloc[0]}
-- Current branch: {repo_df["Current branch"].iloc[0]}
-- Branch list: {repo_df["Branch list"].iloc[0]}
-- Change Overview: {repo_df["Change Overview"].iloc[0]}
-
-Structure:
-{repo_df["Root path"].iloc[0]}
+ğŸ“š Meta data per file:
+{json.dumps(files_info, ensure_ascii=False)}
 
-README summary:
-{json.dumps(readme_summary, ensure_ascii=False)}
+ğŸ“‚ Repository Info:
+- Root: {repo_df["Root path"].iloc[0]}
+- Branch: {repo_df["Current branch"].iloc[0]}
+- Main: {repo_df["Main branch"].iloc[0]}
+- All branches: {repo_df["Branch list"].iloc[0]}
 
-Last 5 commit:
-{json.dumps(recent_commits, ensure_ascii=False)}
+ğŸ“Š Git Change Summary:
+{repo_df["Diff stat"].iloc[0]}
 
-File meta:
-{json.dumps(files_info, ensure_ascii=False)}
+Return ONLY a JSON array (no explanation or comments).
 """
-
     return prompt
 
-# ğŸ¯ ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
+
 def mm_gen_main():
-    repo_df = load_df(REPO_PATH)
-    info_df = load_df(INFO_PATH)
-    strategy_df = load_df(STRATEGY_PATH)
-    prompt_df = init_prompt_df()
-
-    file_list = strategy_df["FILE"].tolist()
-    n = len(file_list)
-
-    if n > 60:
-        raise SystemExit("âš ï¸ ë³€ê²½ FILE ìˆ˜ê°€ 60ê°œ ì´ˆê³¼ â†’ ì‘ì—… ì¢…ë£Œ")
-
-    chunks = (
-        split_chunks(file_list, 3) if n > 50 else
-        split_chunks(file_list, 2) if n > 20 else
-        [file_list]
-    )
-
-    all_results = []
-    enc = tiktoken.encoding_for_model("gpt-4")
-
-    timestamp = datetime.now().strftime("%y%m%d_%H%M")
-    log_dir = f"logs/{timestamp}"
-    os.makedirs(log_dir, exist_ok=True)
-    log_file = f"{log_dir}/trigger.log"
-
-    for chunk in chunks:
-        st_pp_in = build_strategy_pp(repo_df, info_df, strategy_df, chunk)
-
-        in_path = f"{log_dir}/st_pp_in.txt"
-        with open(in_path, "w", encoding="utf-8") as f:
-            f.write(st_pp_in)
-        log(f"âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {in_path}", log_file)
-
-        token_in = len(enc.encode(st_pp_in))
-        prompt_df.loc[len(prompt_df)] = {
-            "IN/OUT": "ì…ë ¥",
-            "VAR NAME": "st_pp_in",
-            "MODEL NAME": "gpt-4o",
-            "meta(in)or purpose(out)": "í´ë” êµ¬ì¡°, README STRATEGY, ë³€ê²½ FILE ëª©ë¡, 5 LATEST COMMIT, ë¸Œëœì¹˜ ì •ë³´, ë³€ê²½ ìš”ì•½ í†µê³„, FILE ìœ í˜•, FILE ìœ„ì¹˜",
-            "SAVE PATH": in_path,
-            "I": False,
-            "upload platform": "",
-            "tokenê°’": token_in,
-            "ë¹„ìš©($)": None,
-            "ë¹„ìš©(krw)": None
-        }
-
-        response = call_llm(prompt=st_pp_in, model="gpt-4o")
-        out_path = f"{log_dir}/st_pp_out.txt"
-        with open(out_path, "w", encoding="utf-8") as f:
-            f.write(response)
-        log(f"âœ… GPT-4o ì‘ë‹µ ì €ì¥ ì™„ë£Œ: {out_path}", log_file)
-
-        token_out = len(enc.encode(response))
-        prompt_df.loc[len(prompt_df)] = {
-            "IN/OUT": "ì¶œë ¥",
-            "VAR NAME": "st_pp_out",
-            "ì‚¬ìš© MODEL NAME": "gpt-4o",
-            "meta(in)or purpose(out)": "strategy_df_value",
-            "SAVE PATH": out_path,
-            "ì—…ë¡œë“œ ì—¬ë¶€": False,
-            "upload platform": "",
-            "tokenê°’": token_out,
-            "ë¹„ìš©($)": None,
-            "ë¹„ìš©(krw)": None
-        }
-
-        parsed = json.loads(response)
-        log("âœ… íŒŒì‹± ì„±ê³µ ë° strategy_df ë°˜ì˜ ì¤‘...", log_file)
-        all_results.extend(parsed)
-
-    for row in all_results:
-        idx = strategy_df[strategy_df["FILE"] == row["FILE"]].index[0]
-        strategy_df.at[idx, "ì‘ì„± ë””í…Œì¼ ë“±ê¸‰"] = row["ì‘ì„± ë””í…Œì¼ ë“±ê¸‰"]
-        strategy_df.at[idx, "ê¸°ëŠ¥ ìœ í˜•"] = row["ê¸°ëŠ¥ ìœ í˜•"]
-        strategy_df.at[idx, "IMPORTANCE"] = row["IMPORTANCE"]
-        strategy_df.at[idx, "ì—°ê´€ë„ ë†’ì€ FILE ë¦¬ìŠ¤íŠ¸"] = row["ì—°ê´€ë„ ë†’ì€ FILE ë¦¬ìŠ¤íŠ¸"]
-
-    save_df(strategy_df, STRATEGY_PATH)
-    save_df(prompt_df, PROMPT_PATH)
-    log("âœ… ì „ëµ ê²°ê³¼ ë° í”„ë¡¬í”„íŠ¸ ì¶”ì  ì €ì¥ ì™„ë£Œ", log_file)
\ No newline at end of file
+    timestamp = cfg.get_timestamp()
+    paths = cfg.get_results_path(timestamp)
+    log_file = cfg.init_log_file(timestamp)
+
+    repo_df = load_df(paths["repo"])
+    info_df = load_df(paths["info"])
+    strategy_df = load_df(paths["strategy"])
+
+    file_list = strategy_df["File"].tolist()
+    if len(file_list) > 60:
+        raise SystemExit("âš ï¸ ë³€ê²½ íŒŒì¼ ìˆ˜ê°€ 60ê°œ ì´ˆê³¼ â†’ ì‘ì—… ì¢…ë£Œ")
+
+    # 1ï¸âƒ£ UUID ë¯¸ë¦¬ ì§€ì •
+    id_map = {file: str(uuid.uuid4()) for file in file_list}
+    for file in file_list:
+        idx = strategy_df[strategy_df["File"] == file].index[0]
+        strategy_df.at[idx, "id"] = id_map[file]
+
+    # 2ï¸âƒ£ Chunk ë¶„í• 
+    for threshold, chunk_count in CHUNK_THRESHOLDS:
+        if len(file_list) > threshold:
+            chunks = split_chunks(file_list, chunk_count)
+            cfg.log(f"ğŸ“¦ {len(file_list)}ê°œ íŒŒì¼ â†’ {chunk_count} ì²­í¬ ë¶„í• ", log_file)
+            break
+    else:
+        chunks = [file_list]
+
+    required_keys = {"id", "File", "Required Commit Detail", "Component Type", "Importance", "Most Related Files"}
+
+    with LLMManager("strategy", repo_df, df_for_call=strategy_df) as llm:
+        for i, chunk in enumerate(chunks):
+            prompt_in = build_strategy_prompt(repo_df, info_df, strategy_df, chunk, id_map)
+
+            name4save = f"chunk_{i+1}"
+            in_path = paths["strategy_in"] / f"in_{i+1}.txt"
+            out_path = paths["strategy_out"] / f"out_{i+1}.txt"
+            in_path.write_text(prompt_in, encoding="utf-8")
+
+            # ğŸ‘‰ ì„ì‹œ ë©”íƒ€ ë°ì´í„° êµ¬ì„±
+            chunk_df = pd.DataFrame([{
+                "id": name4save,
+                "name4save": name4save,
+                "save_path": [str(in_path), str(out_path)]
+            }])
+
+            # ğŸ‘‰ ë‹¨ê±´ì´ë¼ë„ DataFrameìœ¼ë¡œ ë„˜ê¹€ (id ë§¤ì¹­ ê¸°ë°˜)
+            llm.df_for_call = chunk_df
+
+            try:
+                response = llm.call_all([prompt_in], [name4save])[0]
+                out_path.write_text(response, encoding="utf-8")  # â† ë°±ì—…ìš© ì €ì¥ë„ ìœ ì§€
+                parsed = json.loads(clean_llm_response(response))
+
+                for row in parsed:
+                    if not required_keys.issubset(row):
+                        cfg.log(f"âš ï¸ í•„ë“œ ëˆ„ë½ â†’ ë¬´ì‹œë¨: {row}", log_file)
+                        continue
+
+                    idx = strategy_df[strategy_df["id"] == row["id"]].index
+                    if len(idx) == 0:
+                        cfg.log(f"âš ï¸ ì¼ì¹˜í•˜ëŠ” ID ì—†ìŒ: {row['id']}", log_file)
+                        continue
+
+                    i = idx[0]
+                    strategy_df.at[i, "Required Commit Detail"] = row["Required Commit Detail"]
+                    strategy_df.at[i, "Component Type"] = row["Component Type"]
+                    strategy_df.at[i, "Importance"] = row["Importance"]
+                    strategy_df.at[i, "Most Related Files"] = row["Most Related Files"]
+
+            except json.JSONDecodeError as e:
+                cfg.log(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}", log_file)
+                cfg.log(f"ì‘ë‹µ ë‚´ìš©:\n{response}", log_file)
+                raise SystemExit("ğŸš« LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ â†’ JSON í—¤ë” ì œê±° ì—¬ë¶€ í™•ì¸ í•„ìš”")
+            except Exception as e:
+                cfg.log(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}", log_file)
+                raise SystemExit("ğŸš« LLM í˜¸ì¶œ ì‹¤íŒ¨ â†’ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
+
+    save_df(strategy_df, paths["strategy"])
+    cfg.log("âœ… ì „ëµ ê²°ê³¼ ë° í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ", log_file)