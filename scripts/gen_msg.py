from datetime import datetime
import os
import json
import time
import tiktoken
from pathlib import Path
from scripts.dataframe import load_df, save_df, REPO_PATH, INFO_PATH, STRATEGY_PATH, PROMPT_PATH, init_prompt_df
from scripts.llm_router import call_llm

def log(message: str, log_file: Path):
    with log_file.open("a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {message}\n")

def gen_msg_main():
    repo_df = load_df(REPO_PATH)
    info_df = load_df(INFO_PATH)
    strategy_df = load_df(STRATEGY_PATH)
    prompt_df = init_prompt_df()

    # ì„¤ì • ë¡œë”©
    style = json.loads(Path("config/style.json").read_text(encoding="utf-8"))
    llm_cfgs = json.loads(Path("config/llm.json").read_text(encoding="utf-8"))
    commit_style = style["style"]["commit_final"]
    commit_lang = style["language"]["commit"]
    llm_cfg = llm_cfgs["llm"]["commit_final"]

    # ë¡œê·¸ ì„¤ì •
    timestamp = datetime.now().strftime("%y%m%d_%H%M")
    log_dir = Path(f"logs/{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)
    log_file = log_dir / "trigger.log"
    root_path = Path(repo_df["root path"].iloc[0])
    enc = tiktoken.encoding_for_model(llm_cfg["model"][0])

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¯¸ë¦¬ ë¡œë”©
    prompt_template = Path(f"prompt/{commit_lang}/{commit_style}.txt").read_text(encoding="utf-8")

    for row in strategy_df.itertuples():
        filename = row.FILE
        file_path = root_path / "/".join(info_df[info_df["FILE"] == filename]["FILE ìœ„ì¹˜"].iloc[0])
        if not file_path.exists():
            log(f"âŒ FILE ì—†ìŒ: {file_path}", log_file)
            continue

        # ğŸ“„ ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if row.ë¶„ì„_ì „ëµ == "full_pass":
            script_txt = file_path.read_text(encoding="utf-8")
        else:
            lines = file_path.read_text(encoding="utf-8").splitlines()
            keywords = ["def ", "return ", "class ", "self", "@", "from ", "logger"]
            script_txt = "\n".join([line for line in lines if any(k in line for k in keywords)])

        # ğŸ§  ê¸°ëŠ¥ ìš”ì•½ í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        fx_path = log_dir / f"fx_out_{filename}.txt"
        if not fx_path.exists():
            log(f"âš ï¸ fx_out FILE ì—†ìŒ: {fx_path}", log_file)
            continue
        fx_summary = fx_path.read_text(encoding="utf-8")

        # ğŸ§¾ diff í…ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
        diff_var = info_df[info_df["FILE"] == filename]["diff var name"].iloc[0]
        diff_path = Path(f"results/diff_final/{diff_var}.txt")
        if not diff_path.exists():
            log(f"âš ï¸ diff í…ìŠ¤íŠ¸ ì—†ìŒ: {diff_path}", log_file)
            continue
        diff_txt = diff_path.read_text(encoding="utf-8")

        # ğŸ“Œ ìµœê·¼ ì»¤ë°‹ ë©”ì‹œì§€
        commit_msgs = info_df[info_df["FILE"] == filename]["5 LATEST COMMIT"].iloc[0]
        recent_commit = "\n".join(commit_msgs[:row.ì¶”ì¶œí• _ì»¤ë°‹_ë©”ì‹œì§€_ê°œìˆ˜])

        # ğŸ—‚ï¸ í´ë” êµ¬ì¡°
        tree_txt_path = Path("results/context/tree.txt")
        tree_txt = tree_txt_path.read_text(encoding="utf-8") if tree_txt_path.exists() else ""

        # ğŸ§¾ í”„ë¡¬í”„íŠ¸ ìƒì„±
        full_prompt = prompt_template.replace("{change}", f"""
ğŸ“˜ ê¸°ëŠ¥ ìš”ì•½:
{fx_summary}

ğŸ“‚ í´ë” êµ¬ì¡°:
{tree_txt}

ğŸ“„ ë³€ê²½ëœ ìŠ¤í¬ë¦½íŠ¸ ì£¼ìš” ë‚´ìš©:
{script_txt}

ğŸ“Œ ìµœê·¼ ì»¤ë°‹ ë©”ì‹œì§€:
{recent_commit}

ğŸ§¾ ë³€ê²½ ì‚¬í•­(diff):
{diff_txt}
""").strip()

        # ğŸ” í”„ë¡¬í”„íŠ¸ ì €ì¥ ë° ì¶”ì 
        prompt_file = log_dir / f"commit_in_{filename}.txt"
        prompt_file.write_text(full_prompt, encoding="utf-8")
        token_in = len(enc.encode(full_prompt))
        prompt_df.loc[len(prompt_df)] = {
            "IN/OUT": "ì…ë ¥", "VAR NAME": f"commit_in_{filename}",
            "ì‚¬ìš© MODEL NAME": llm_cfg["model"][0],
            "meta(in)or purpose(out)": "ê¸°ëŠ¥ ìš”ì•½, í´ë” êµ¬ì¡°, ìµœê·¼ ì»¤ë°‹ ë©”ì‹œì§€, ë³€ê²½ ìŠ¤í¬ë¦½íŠ¸, diff",
            "SAVE PATH": str(prompt_file), "ì—…ë¡œë“œ ì—¬ë¶€": False,
            "upload platform": "", "tokenê°’": token_in,
            "ë¹„ìš©($)": None, "ë¹„ìš©(krw)": None
        }
        log(f"âœ… ì»¤ë°‹ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {prompt_file}", log_file)

        # LLM í˜¸ì¶œ
        response = call_llm(prompt=full_prompt, llm_cfg=llm_cfg, log=lambda m: log(m, log_file))
        result_file = log_dir / f"commit_out_{filename}.txt"
        result_file.write_text(response, encoding="utf-8")
        token_out = len(enc.encode(response))

        prompt_df.loc[len(prompt_df)] = {
            "IN/OUT": "ì¶œë ¥", "VAR NAME": f"commit_out_{filename}",
            "ì‚¬ìš© MODEL NAME": llm_cfg["model"][0],
            "meta(in)or purpose(out)": "ìµœì¢… ì»¤ë°‹ ë©”ì‹œì§€ ìƒì„±",
            "SAVE PATH": str(result_file), "ì—…ë¡œë“œ ì—¬ë¶€": True,
            "upload platform": ["notify", "record"],
            "tokenê°’": token_out, "ë¹„ìš©($)": None, "ë¹„ìš©(krw)": None
        }
        log(f"âœ… ì»¤ë°‹ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {result_file}", log_file)

        save_df(prompt_df, PROMPT_PATH)
        time.sleep(5)

gen_msg_main()
